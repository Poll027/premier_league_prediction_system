{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa02b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62688a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBrefBaseScraper:\n",
    "    \"\"\"Base scraper class with common functionality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.base_url = \"https://fbref.com\"\n",
    "        \n",
    "    def get_soup(self, url: str, delay: float = 1.0) -> BeautifulSoup:\n",
    "        \"\"\"Get BeautifulSoup object from URL with rate limiting\"\"\"\n",
    "        time.sleep(delay)\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_table_data(self, soup: BeautifulSoup, table_selector: str) -> pd.DataFrame:\n",
    "        \"\"\"Generic table extraction method\"\"\"\n",
    "        if not soup:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Try different ways to find the table\n",
    "        table = soup.find('table', {'id': table_selector})\n",
    "        if not table:\n",
    "            table = soup.find('table', class_='stats_table')\n",
    "        if not table:\n",
    "            tables = soup.find_all('table')\n",
    "            table = tables[0] if tables else None\n",
    "            \n",
    "        if not table:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Extract headers\n",
    "        headers = []\n",
    "        thead = table.find('thead')\n",
    "        if thead:\n",
    "            header_rows = thead.find_all('tr')\n",
    "            # Get the most complete header row\n",
    "            for row in reversed(header_rows):\n",
    "                row_headers = [th.get_text(strip=True) for th in row.find_all(['th', 'td'])]\n",
    "                if len(row_headers) > len(headers):\n",
    "                    headers = row_headers\n",
    "        \n",
    "        # Extract data rows\n",
    "        tbody = table.find('tbody')\n",
    "        rows_data = []\n",
    "        if tbody:\n",
    "            for row in tbody.find_all('tr'):\n",
    "                if row.get('class') and any(cls in ['spacer', 'thead'] for cls in row.get('class')):\n",
    "                    continue\n",
    "                    \n",
    "                row_data = []\n",
    "                for cell in row.find_all(['td', 'th']):\n",
    "                    row_data.append(cell.get_text(strip=True))\n",
    "                \n",
    "                if len(row_data) > 1:\n",
    "                    rows_data.append(row_data)\n",
    "        \n",
    "        if not rows_data or not headers:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Ensure consistent column count\n",
    "        max_cols = max(len(headers), max(len(row) for row in rows_data) if rows_data else 0)\n",
    "        headers = headers[:max_cols] + [f'Col_{i}' for i in range(len(headers), max_cols)]\n",
    "        \n",
    "        for i, row in enumerate(rows_data):\n",
    "            rows_data[i] = row[:max_cols] + [''] * (max_cols - len(row))\n",
    "        \n",
    "        return pd.DataFrame(rows_data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02925583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeasonTablesScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper for league tables across multiple seasons\"\"\"\n",
    "    \n",
    "    def get_season_urls(self, years_back: int = 10) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Get URLs for league tables for the last N seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        season_urls = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            season_url = f\"{self.base_url}/en/comps/9/{season}/stats/{season}-Premier-League-Stats\"\n",
    "            season_urls.append((season, season_url))\n",
    "        \n",
    "        return season_urls\n",
    "    \n",
    "    def scrape_season_table(self, season: str, url: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape league table for a specific season\"\"\"\n",
    "        print(f\"Scraping league table for {season}...\")\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        df = self.extract_table_data(soup, 'stats_standard_combined')\n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_seasons(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape league tables for all seasons\"\"\"\n",
    "        season_urls = self.get_season_urls(years_back)\n",
    "        all_tables = []\n",
    "        \n",
    "        for season, url in season_urls:\n",
    "            table_data = self.scrape_season_table(season, url)\n",
    "            if not table_data.empty:\n",
    "                all_tables.append(table_data)\n",
    "        \n",
    "        return pd.concat(all_tables, ignore_index=True) if all_tables else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 2:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape league tables only\n",
    "    table_scraper = SeasonTablesScraper()\n",
    "    league_tables = table_scraper.scrape_all_seasons(years_back=5)\n",
    "    league_tables.to_csv('league_tables.csv', index=False)\n",
    "    print(f\"Saved {len(league_tables)} league table records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c67ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixturesScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper for match fixtures and results\"\"\"\n",
    "    \n",
    "    def get_fixtures_urls(self, years_back: int = 10) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Get fixture URLs for multiple seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        fixture_urls = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            fixtures_url = f\"{self.base_url}/en/comps/9/{season}/schedule/{season}-Premier-League-Scores-and-Fixtures\"\n",
    "            fixture_urls.append((season, fixtures_url))\n",
    "        \n",
    "        return fixture_urls\n",
    "    \n",
    "    def scrape_season_fixtures(self, season: str, url: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape all fixtures for a season\"\"\"\n",
    "        print(f\"Scraping fixtures for {season}...\")\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        # Try multiple table IDs that FBref uses for fixtures\n",
    "        table_ids = [f'sched_ks_17358_1', 'sched_2025_17358_1', 'fixtures']\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for table_id in table_ids:\n",
    "            df = self.extract_table_data(soup, table_id)\n",
    "            if not df.empty:\n",
    "                break\n",
    "        \n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_fixtures(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape fixtures for all seasons\"\"\"\n",
    "        fixture_urls = self.get_fixtures_urls(years_back)\n",
    "        all_fixtures = []\n",
    "        \n",
    "        for season, url in fixture_urls:\n",
    "            fixtures_data = self.scrape_season_fixtures(season, url)\n",
    "            if not fixtures_data.empty:\n",
    "                all_fixtures.append(fixtures_data)\n",
    "        \n",
    "        return pd.concat(all_fixtures, ignore_index=True) if all_fixtures else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 3:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape fixtures only\n",
    "    fixtures_scraper = FixturesScraper()\n",
    "    fixtures = fixtures_scraper.scrape_all_fixtures(years_back=5)\n",
    "    fixtures.to_csv('fixtures.csv', index=False)\n",
    "    print(f\"Saved {len(fixtures)} fixture records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShootingStatsScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper specifically for team shooting statistics\"\"\"\n",
    "    \n",
    "    def get_shooting_url(self, season: str) -> str:\n",
    "        \"\"\"Build URL for shooting stats\"\"\"\n",
    "        return f\"{self.base_url}/en/comps/9/{season}/shooting/{season}-Premier-League-Stats\"\n",
    "    \n",
    "    def scrape_team_shooting(self, season: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape team shooting stats for a season\"\"\"\n",
    "        print(f\"Scraping shooting stats for {season}...\")\n",
    "        url = self.get_shooting_url(season)\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        df = self.extract_table_data(soup, 'stats_shooting')\n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_shooting_stats(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape shooting stats for all seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        all_shooting = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            \n",
    "            shooting_data = self.scrape_team_shooting(season)\n",
    "            if not shooting_data.empty:\n",
    "                all_shooting.append(shooting_data)\n",
    "        \n",
    "        return pd.concat(all_shooting, ignore_index=True) if all_shooting else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 4:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape shooting stats only\n",
    "    shooting_scraper = ShootingStatsScraper()\n",
    "    shooting_stats = shooting_scraper.scrape_all_shooting_stats(years_back=5)\n",
    "    shooting_stats.to_csv('team_shooting_stats.csv', index=False)\n",
    "    print(f\"Saved {len(shooting_stats)} shooting stat records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd13952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassingStatsScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper specifically for team passing statistics\"\"\"\n",
    "    \n",
    "    def get_passing_url(self, season: str) -> str:\n",
    "        \"\"\"Build URL for passing stats\"\"\"\n",
    "        return f\"{self.base_url}/en/comps/9/{season}/passing/{season}-Premier-League-Stats\"\n",
    "    \n",
    "    def scrape_team_passing(self, season: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape team passing stats for a season\"\"\"\n",
    "        print(f\"Scraping passing stats for {season}...\")\n",
    "        url = self.get_passing_url(season)\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        df = self.extract_table_data(soup, 'stats_passing')\n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_passing_stats(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape passing stats for all seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        all_passing = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            \n",
    "            passing_data = self.scrape_team_passing(season)\n",
    "            if not passing_data.empty:\n",
    "                all_passing.append(passing_data)\n",
    "        \n",
    "        return pd.concat(all_passing, ignore_index=True) if all_passing else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 5:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape passing stats only\n",
    "    passing_scraper = PassingStatsScraper()\n",
    "    passing_stats = passing_scraper.scrape_all_passing_stats(years_back=5)\n",
    "    passing_stats.to_csv('team_passing_stats.csv', index=False)\n",
    "    print(f\"Saved {len(passing_stats)} passing stat records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefensiveStatsScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper specifically for team defensive statistics\"\"\"\n",
    "    \n",
    "    def get_defense_url(self, season: str) -> str:\n",
    "        \"\"\"Build URL for defensive stats\"\"\"\n",
    "        return f\"{self.base_url}/en/comps/9/{season}/defense/{season}-Premier-League-Stats\"\n",
    "    \n",
    "    def scrape_team_defense(self, season: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape team defensive stats for a season\"\"\"\n",
    "        print(f\"Scraping defensive stats for {season}...\")\n",
    "        url = self.get_defense_url(season)\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        df = self.extract_table_data(soup, 'stats_defense')\n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_defensive_stats(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape defensive stats for all seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        all_defense = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            \n",
    "            defense_data = self.scrape_team_defense(season)\n",
    "            if not defense_data.empty:\n",
    "                all_defense.append(defense_data)\n",
    "        \n",
    "        return pd.concat(all_defense, ignore_index=True) if all_defense else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 6:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape defensive stats only\n",
    "    defense_scraper = DefensiveStatsScraper()\n",
    "    defense_stats = defense_scraper.scrape_all_defensive_stats(years_back=5)\n",
    "    defense_stats.to_csv('team_defensive_stats.csv', index=False)\n",
    "    print(f\"Saved {len(defense_stats)} defensive stat records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PossessionStatsScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper specifically for team possession statistics\"\"\"\n",
    "    \n",
    "    def get_possession_url(self, season: str) -> str:\n",
    "        \"\"\"Build URL for possession stats\"\"\"\n",
    "        return f\"{self.base_url}/en/comps/9/{season}/possession/{season}-Premier-League-Stats\"\n",
    "    \n",
    "    def scrape_team_possession(self, season: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape team possession stats for a season\"\"\"\n",
    "        print(f\"Scraping possession stats for {season}...\")\n",
    "        url = self.get_possession_url(season)\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        df = self.extract_table_data(soup, 'stats_possession')\n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_possession_stats(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape possession stats for all seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        all_possession = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            \n",
    "            possession_data = self.scrape_team_possession(season)\n",
    "            if not possession_data.empty:\n",
    "                all_possession.append(possession_data)\n",
    "        \n",
    "        return pd.concat(all_possession, ignore_index=True) if all_possession else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 7:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape possession stats only\n",
    "    possession_scraper = PossessionStatsScraper()\n",
    "    possession_stats = possession_scraper.scrape_all_possession_stats(years_back=5)\n",
    "    possession_stats.to_csv('team_possession_stats.csv', index=False)\n",
    "    print(f\"Saved {len(possession_stats)} possession stat records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff443e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalkeepingStatsScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper specifically for goalkeeping statistics\"\"\"\n",
    "    \n",
    "    def get_goalkeeping_url(self, season: str) -> str:\n",
    "        \"\"\"Build URL for goalkeeping stats\"\"\"\n",
    "        return f\"{self.base_url}/en/comps/9/{season}/goalkeeping/{season}-Premier-League-Stats\"\n",
    "    \n",
    "    def scrape_team_goalkeeping(self, season: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape team goalkeeping stats for a season\"\"\"\n",
    "        print(f\"Scraping goalkeeping stats for {season}...\")\n",
    "        url = self.get_goalkeeping_url(season)\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        df = self.extract_table_data(soup, 'stats_keeper')\n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_goalkeeping_stats(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape goalkeeping stats for all seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        all_goalkeeping = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            \n",
    "            goalkeeping_data = self.scrape_team_goalkeeping(season)\n",
    "            if not goalkeeping_data.empty:\n",
    "                all_goalkeeping.append(goalkeeping_data)\n",
    "        \n",
    "        return pd.concat(all_goalkeeping, ignore_index=True) if all_goalkeeping else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 8:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape goalkeeping stats only\n",
    "    gk_scraper = GoalkeepingStatsScraper()\n",
    "    gk_stats = gk_scraper.scrape_all_goalkeeping_stats(years_back=5)\n",
    "    gk_stats.to_csv('team_goalkeeping_stats.csv', index=False)\n",
    "    print(f\"Saved {len(gk_stats)} goalkeeping stat records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94034778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayerStatsScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper for individual player statistics\"\"\"\n",
    "    \n",
    "    def get_player_stats_url(self, season: str) -> str:\n",
    "        \"\"\"Get URL for player stats\"\"\"\n",
    "        return f\"{self.base_url}/en/comps/9/{season}/stats/{season}-Premier-League-Stats\"\n",
    "    \n",
    "    def scrape_player_stats(self, season: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape individual player statistics\"\"\"\n",
    "        print(f\"Scraping player stats for {season}...\")\n",
    "        url = self.get_player_stats_url(season)\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        # Look for player stats table (usually has different structure than team stats)\n",
    "        df = self.extract_table_data(soup, 'stats_standard')\n",
    "        \n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_player_stats(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape player stats for multiple seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        all_player_data = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            \n",
    "            player_data = self.scrape_player_stats(season)\n",
    "            if not player_data.empty:\n",
    "                all_player_data.append(player_data)\n",
    "        \n",
    "        return pd.concat(all_player_data, ignore_index=True) if all_player_data else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 9:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape player stats only\n",
    "    player_scraper = PlayerStatsScraper()\n",
    "    player_stats = player_scraper.scrape_all_player_stats(years_back=3)\n",
    "    player_stats.to_csv('player_stats.csv', index=False)\n",
    "    print(f\"Saved {len(player_stats)} player stat records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferDataScraper(FBrefBaseScraper):\n",
    "    \"\"\"Scraper for transfer market data\"\"\"\n",
    "    \n",
    "    def get_transfers_url(self, season: str) -> str:\n",
    "        \"\"\"Get URL for transfer data\"\"\"\n",
    "        return f\"{self.base_url}/en/comps/9/{season}/transfers/{season}-Premier-League-Transfers\"\n",
    "    \n",
    "    def scrape_season_transfers(self, season: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape transfer data for a season\"\"\"\n",
    "        print(f\"Scraping transfers for {season}...\")\n",
    "        url = self.get_transfers_url(season)\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        # Look for transfers table\n",
    "        df = self.extract_table_data(soup, 'transfers')\n",
    "        \n",
    "        if not df.empty:\n",
    "            df['season'] = season\n",
    "            df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_transfers(self, years_back: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Scrape transfer data for multiple seasons\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        all_transfers = []\n",
    "        \n",
    "        for i in range(years_back):\n",
    "            season_start = current_year - i - 1\n",
    "            season_end = season_start + 1\n",
    "            season = f\"{season_start}-{season_end}\"\n",
    "            \n",
    "            transfers = self.scrape_season_transfers(season)\n",
    "            if not transfers.empty:\n",
    "                all_transfers.append(transfers)\n",
    "        \n",
    "        return pd.concat(all_transfers, ignore_index=True) if all_transfers else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for Block 10:\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape transfer data only\n",
    "    transfer_scraper = TransferDataScraper()\n",
    "    transfer_data = transfer_scraper.scrape_all_transfers(years_back=5)\n",
    "    transfer_data.to_csv('transfers.csv', index=False)\n",
    "    print(f\"Saved {len(transfer_data)} transfer records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef13479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveDataScraper:\n",
    "    \"\"\"Master class that coordinates all data collection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.season_tables_scraper = SeasonTablesScraper()\n",
    "        self.fixtures_scraper = FixturesScraper()\n",
    "        self.shooting_scraper = ShootingStatsScraper()\n",
    "        self.passing_scraper = PassingStatsScraper()\n",
    "        self.defense_scraper = DefensiveStatsScraper()\n",
    "        self.possession_scraper = PossessionStatsScraper()\n",
    "        self.goalkeeping_scraper = GoalkeepingStatsScraper()\n",
    "        self.player_scraper = PlayerStatsScraper()\n",
    "        self.transfer_scraper = TransferDataScraper()\n",
    "    \n",
    "    def scrape_all_data(self, years_back: int = 10, include_players: bool = True, \n",
    "                       include_transfers: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Scrape all available data for match prediction\"\"\"\n",
    "        \n",
    "        print(f\"Starting comprehensive data scraping for last {years_back} seasons...\")\n",
    "        all_data = {}\n",
    "        \n",
    "        # Core match data\n",
    "        print(\"\\n=== SCRAPING CORE DATA ===\")\n",
    "        all_data['season_tables'] = self.season_tables_scraper.scrape_all_seasons(years_back)\n",
    "        all_data['fixtures'] = self.fixtures_scraper.scrape_all_fixtures(years_back)\n",
    "        \n",
    "        # Team statistics\n",
    "        print(\"\\n=== SCRAPING TEAM STATS ===\")\n",
    "        all_data['team_shooting'] = self.shooting_scraper.scrape_all_shooting_stats(years_back)\n",
    "        all_data['team_passing'] = self.passing_scraper.scrape_all_passing_stats(years_back)\n",
    "        all_data['team_defense'] = self.defense_scraper.scrape_all_defensive_stats(years_back)\n",
    "        all_data['team_possession'] = self.possession_scraper.scrape_all_possession_stats(years_back)\n",
    "        all_data['team_goalkeeping'] = self.goalkeeping_scraper.scrape_all_goalkeeping_stats(years_back)\n",
    "        \n",
    "        # Optional data\n",
    "        if include_players:\n",
    "            print(\"\\n=== SCRAPING PLAYER DATA ===\")\n",
    "            all_data['player_stats'] = self.player_scraper.scrape_all_player_stats(years_back)\n",
    "        \n",
    "        if include_transfers:\n",
    "            print(\"\\n=== SCRAPING TRANSFER DATA ===\")\n",
    "            all_data['transfers'] = self.transfer_scraper.scrape_all_transfers(years_back)\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def save_all_data(self, data: Dict[str, pd.DataFrame], output_dir: str = \"data/\"):\n",
    "        \"\"\"Save all scraped data to CSV files\"\"\"\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for data_name, df in data.items():\n",
    "            if not df.empty:\n",
    "                filepath = os.path.join(output_dir, f\"{data_name}.csv\")\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"Saved {data_name}: {len(df)} records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
